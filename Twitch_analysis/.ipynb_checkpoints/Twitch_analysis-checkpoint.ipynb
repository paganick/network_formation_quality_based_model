{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "©2020-2021 ETH Zurich, Pagan Nicolò; D-ITET; Automatic Control Lab\n",
    "\n",
    "This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.\n",
    "\n",
    "This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.\n",
    "\n",
    "You should have received a copy of the GNU General Public License along with this program.  If not, see <https://www.gnu.org/licenses/>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib import cm    \n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import tikzplotlib as tikz\n",
    "from scipy import interpolate, stats\n",
    "import powerlaw\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"]=[10, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function finds the best linear log-log regression, and its goodness of fit parameters.\n",
    "def linear_reg_log(x, y):\n",
    "    print('Power law fit')\n",
    "    x_log = np.log(x)\n",
    "    y_log = np.log(y)\n",
    "    pearson_coeff, p_value = stats.pearsonr(x_log, y_log)\n",
    "    \n",
    "    fit_line = np.poly1d(np.polyfit(x_log, y_log, deg=1))\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(x_log, y_log)\n",
    "    print ('slope:', slope, 'intercept:', intercept, 'r_value:', r_value, 'p_value:', p_value, 'std_err:', std_err)\n",
    "    \n",
    "    yhat = np.polyval(fit_line,x_log)\n",
    "    SSE= sum((y_log-yhat)**2)\n",
    "    MSE = SSE/len(y_log)\n",
    "    RMSE = np.sqrt(MSE)\n",
    "    print ('RMSE', RMSE)\n",
    "    TSS = sum((y_log-np.mean(y_log))**2)     \n",
    "    r_squared = 1 - (float(SSE))/TSS\n",
    "    print ('r2', r_squared)\n",
    "    adjusted_r_squared = 1 - (1-r_squared)*(len(y_log)-1)/(len(y_log)-1-1)\n",
    "    print ('adjusted r2', adjusted_r_squared)\n",
    "    y_fit = np.exp(yhat)\n",
    "    return pearson_coeff, fit_line, y_fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function is used to make the colormap scale non-linear \n",
    "def scaled_cmap(original_map, scalingIntensity):\n",
    "    R = []\n",
    "    G = []\n",
    "    B = [] \n",
    "    for i in range(255):\n",
    "        R.append(original_map(i)[0])\n",
    "        G.append(original_map(i)[1])\n",
    "        B.append(original_map(i)[2])\n",
    "    dataMax = 1;\n",
    "    dataMin = 0;\n",
    "    centerPoint = 0;\n",
    "    x = np.linspace(1,255, 255)\n",
    "    x = x - (centerPoint-dataMin)*len(x)/(dataMax-dataMin)\n",
    "    x = scalingIntensity * x/np.max(np.abs(x))\n",
    "    x = np.sign(x)* np.exp(np.abs(x))\n",
    "    x = x - min(x); \n",
    "    x = x*511/max(x)+1\n",
    "    fR = interpolate.interp1d(x, R)\n",
    "    fG = interpolate.interp1d(x, G)\n",
    "    fB = interpolate.interp1d(x, B)\n",
    "    colormapR = fR(np.linspace(1,512,512))\n",
    "    colormapG = fG(np.linspace(1,512,512))\n",
    "    colormapB = fB(np.linspace(1,512,512))\n",
    "    colormap = np.zeros([512,3])\n",
    "    for i in range(512):\n",
    "        colormap[i] = [colormapR[i], colormapG[i], colormapB[i]]\n",
    "    colormap[-1] = [1, 1, 1]\n",
    "    return LinearSegmentedColormap.from_list('mycmap', colormap, N=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game:\n",
    "    # To initialize the module, we need the name of the data-set, namely the name of the database, as well as the filtering criterion which consists of:\n",
    "    # the dedication, i.e., the interest index, as defined in the main text\n",
    "    # the final_date: we only consider nodes and edges created within the chosen date (01.10.2020)\n",
    "    def __init__(self, name, folder_name, filtering):\n",
    "        self.name = name\n",
    "        self.folder_name = folder_name\n",
    "        self.dedication = filtering['dedication']\n",
    "        self.final_date = filtering['final_date']\n",
    "        self.game_db = '{}.db'.format(name)\n",
    "        self.n_nodes = 0\n",
    "        self.n_edges = 0\n",
    "        self.n_followers = 0\n",
    "        if not os.path.exists(folder_name):\n",
    "                os.makedirs(folder_name)\n",
    "\n",
    "    \n",
    "    # The following function gets the data from the database, which contains a user_db, and an edge_db\n",
    "    # Entries can be removed, if the id is provided in the id_remove_list.\n",
    "    def fetch_data(self, id_remove_list):\n",
    "        conn = sqlite3.connect(self.game_db)\n",
    "        self.nodes_df = pd.read_sql_query(\"SELECT id, created_at, dedication FROM user_db\", conn)\n",
    "        self.edges_df = pd.read_sql_query(\"SELECT source,target,date FROM edges_db\", conn)\n",
    "        self.nodes_df = self.nodes_df.loc[(self.nodes_df['dedication']>=self.dedication) & (pd.to_datetime(self.nodes_df['created_at']) <self.final_date) ]\n",
    "        self.nodes_df = self.nodes_df[~self.nodes_df['Id'].isin(id_remove_list)]\n",
    "\n",
    "        self.edges_df = self.edges_df.loc[(pd.to_datetime(self.edges_df['Date']) < self.final_date) & \n",
    "            (self.edges_df['Target'].isin(self.nodes_df.loc[:,'Id'].values))]\n",
    "        self.edges_df.sort_values('Date', inplace = True)\n",
    "        self.n_edges = self.edges_df.shape[0]\n",
    "        self.outdegree_struct = Counter(elem for elem in tuple(self.edges_df['Source']))\n",
    "        \n",
    "        self.followers_df = self.edges_df['Source'].value_counts().rename_axis('Id').reset_index(name='Outdegree')\n",
    "        self.n_followers = self.followers_df.shape[0]\n",
    "        \n",
    "        self.compute_indegree()\n",
    "        self.n_nodes = len(self.nodes_df.index)\n",
    "        \n",
    "    # The nect function computes some basic statistics on the data-set    \n",
    "    def print_statistics(self):\n",
    "        print('Number of broadcasters:',  self.n_nodes)\n",
    "        print('Number of edges:', self.n_edges)\n",
    "        print('Number of followers:', self.n_followers)\n",
    "        print('First edge: ' + str(pd.to_datetime(self.edges_df['Date'].head(1).item())))\n",
    "        print('Last edge: '+ str(pd.to_datetime(self.edges_df['Date'].tail(1).item())))\n",
    "     \n",
    "    # The following function computes and stores the in-degree of the users in the database.\n",
    "    def compute_indegree(self):\n",
    "        indegree = self.edges_df['Target'].value_counts().rename_axis('Id').reset_index(name='Indegree')\n",
    "        self.nodes_df = pd.merge(self.nodes_df, indegree, on='Id')\n",
    "        self.nodes_df['Rank'] = self.nodes_df['Indegree'].rank(ascending=False)\n",
    "        self.nodes_df.sort_values('Rank', inplace = True)    \n",
    "    \n",
    "    # The following function plots the in-degree as a function of the rank, for the first n_top_agents.\n",
    "    # The data are fitted with a linear regression (after log-log conversion).\n",
    "    def plot_Zipf(self, n_top_agents, save_plot):\n",
    "        rank = self.nodes_df['Rank'][0:n_top_agents]\n",
    "        indegree = self.nodes_df['Indegree'][0:n_top_agents]\n",
    "        f_pearson_coeff, fit_line, y_fit = linear_reg_log(rank, indegree)  \n",
    "        \n",
    "        plt.figure()\n",
    "        plt.plot(rank, indegree, 'o', markersize=2, label='origin')\n",
    "        plt.plot(rank, y_fit,\n",
    "               label=\"Slope={:.2f}, \".format(fit_line[1]) + 'Pearson coeff={:.2f}'.format(f_pearson_coeff))\n",
    "        plt.yscale('log')\n",
    "        plt.xscale('log')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.xlabel('Rank')\n",
    "        plt.ylabel('Indegree')\n",
    "        plt.title(self.name + ': top {} broadcasters'.format(n_top_agents))\n",
    "        if (save_plot):\n",
    "            tikz.save(self.folder_name+'Zipf.tikz', table_row_sep='\\\\\\\\\\n')\n",
    "\n",
    "       \n",
    "    # In-degree analysis functions:\n",
    "    \n",
    "    # The following function plots the in-degree probability density function, and its power-law fit(s) and lognormal fit.\n",
    "    def plot_indegree_pdf(self, save_plot):\n",
    "        indegree = self.nodes_df['Indegree']\n",
    "        xmax = max(indegree)\n",
    "        xmin = min(indegree)\n",
    "        \n",
    "        n=len(indegree)\n",
    "        zipfs_law_x = range(1,n)\n",
    "        zipfs_law_y = xmax* np.power(np.linspace(1,n,n), -1)\n",
    "        \n",
    "        powerlawfit = powerlaw.Fit(indegree, discrete=True, estimate_discrete=True, fit_method='Likelihood')\n",
    "        # As alternative, we also considere a power-law fit in which we define the xmin.\n",
    "        powerlawfit_reduced = powerlaw.Fit(indegree, xmin=18000, discrete=True, estimate_discrete=True, fit_method='Likelihood') \n",
    "        xmin_PL = int(powerlawfit.power_law.xmin)\n",
    "        xmin_PL_reduced = int(powerlawfit_reduced.power_law.xmin)\n",
    "        powerlaw_fit_distribution = powerlaw.Power_Law(xmin=xmin_PL, parameters=[powerlawfit.alpha], discrete=True)  \n",
    "        powerlaw_fit_reduced_distribution = powerlaw.Power_Law(xmin=xmin_PL_reduced, parameters=[powerlawfit_reduced.alpha], discrete=True)\n",
    "        \n",
    "        nbins = 50\n",
    "        bins = np.logspace(np.log10(10), np.log10(xmax+1), nbins)\n",
    "        widths = (bins[1:] - bins[:-1])\n",
    "        hist = np.histogram(indegree, bins=bins)\n",
    "        hist_norm = hist[0]/widths\n",
    "        plt.scatter(bins[:-1], hist_norm, s=25, color='r', marker='o', label='Empirical data', alpha=0.5)\n",
    "        plt.bar(bins[:-1], hist_norm, widths, bottom=10**(-6), alpha=0.5, label='Empirical histogram')\n",
    "          \n",
    "        hist_zipf = np.histogram(zipfs_law_y, bins=bins)\n",
    "        hist_zipf_norm = hist_zipf[0]/widths\n",
    "        plt.scatter(bins[:-1], hist_zipf_norm, s=25,  color='green', marker='o', label='Zipf\\'s law', alpha=0.5)\n",
    "        \n",
    "        n_indegree = len(indegree)\n",
    "        n_power_law = len(indegree[indegree>=xmin_PL])\n",
    "        n_power_law_reduced = len(indegree[indegree>=xmin_PL_reduced])\n",
    "        n_samples = 1000000\n",
    "        \n",
    "        powerlaw_random = powerlaw_fit_distribution.generate_random(n_samples)\n",
    "        hist_powerlaw = np.histogram(powerlaw_random, bins=bins)\n",
    "        hist_powerlaw_norm = hist_powerlaw[0]/widths/n_samples*n_power_law\n",
    "        plt.loglog(bins[1:-1], hist_powerlaw_norm[1:], alpha=0.5,  label='fit pdf: alpha=' +str(powerlawfit.power_law.alpha)+' sigma='+str(powerlawfit.power_law.sigma)+' xmin='+str(int(powerlawfit.power_law.xmin)))\n",
    "        \n",
    "        powerlaw_reduced_random = powerlaw_fit_reduced_distribution.generate_random(n_samples)\n",
    "        hist_powerlaw_reduced = np.histogram(powerlaw_reduced_random, bins=bins)\n",
    "        hist_powerlaw_reduced_norm = hist_powerlaw_reduced[0]/widths/n_samples*n_power_law_reduced\n",
    "        plt.loglog(bins[1:-1], hist_powerlaw_reduced_norm[1:], alpha=0.5,  label='fit pdf: alpha=' +str(powerlawfit_reduced.power_law.alpha)+' sigma='+str(powerlawfit_reduced.power_law.sigma)+' xmin='+str(int(powerlawfit_reduced.power_law.xmin)))\n",
    "        \n",
    "        \n",
    "        sigma, loc, scale = stats.lognorm.fit(indegree, floc=0)\n",
    "        logn_samples = np.random.lognormal(np.log(scale), sigma, n_samples)\n",
    "        hist_logn = np.histogram(logn_samples, bins=bins)\n",
    "        hist_logn_norm = hist_logn[0]/widths/len(logn_samples)*n_indegree\n",
    "        plt.loglog(bins[:-1], hist_logn_norm, alpha=0.5,  label='lognormal fit mu='+str(np.log(scale))+' sigma='+str(sigma))\n",
    "        plt.title('Indegree Probability Density Function and power-law fit, normalized after xmin')\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.xlabel('indegree')\n",
    "        plt.ylabel('pdf')\n",
    "        if (save_plot):\n",
    "             tikz.save(self.folder_name+'indegree_pdf.tikz', table_row_sep='\\\\\\\\\\n')\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "    # The following function plots the in-degree complementary cumulative distribution function, and its power-law fit(s) and lognormal fit.    \n",
    "    def plot_indegree_ccdf(self, save_plot):\n",
    "        indegree = self.nodes_df['Indegree']\n",
    "        xmax = max(indegree)\n",
    "        xmin = min(indegree)\n",
    "        \n",
    "        n=len(indegree)\n",
    "        zipfs_law_x = range(1,n)\n",
    "        zipfs_law_y = xmax* np.power(np.linspace(1,n,n), -1)\n",
    "        \n",
    "        \n",
    "        powerlawfit = powerlaw.Fit(indegree, discrete=True, estimate_discrete=True, fit_method='Likelihood')\n",
    "        # As alternative, we also considere a power-law fit in which we define the xmin.\n",
    "        powerlawfit_reduced = powerlaw.Fit(indegree, xmin=18000, discrete=True, estimate_discrete=True, fit_method='Likelihood') \n",
    "        xmin_PL = int(powerlawfit.power_law.xmin)\n",
    "        xmin_PL_reduced = int(powerlawfit_reduced.power_law.xmin)\n",
    "        powerlaw_fit_distribution = powerlaw.Power_Law(xmin=xmin_PL, parameters=[powerlawfit.alpha], discrete=True) # xmin=2 might not be good for different values of N. This is a bit of a hack, because of the powerlaw interpolation package \n",
    "        powerlaw_fit_reduced_distribution = powerlaw.Power_Law(xmin=xmin_PL_reduced, parameters=[powerlawfit_reduced.alpha], discrete=True) # xmin=2 might not be good for different values of N. This is a bit of a hack, because of the powerlaw interpolation package \n",
    "     \n",
    "        nbins = 100\n",
    "        bins = np.logspace(np.log10(xmin), np.log10(xmax+1), nbins)\n",
    "        widths = (bins[1:] - bins[:-1])\n",
    "        hist = np.histogram(indegree, bins=bins)\n",
    "        hist_norm = hist[0]/sum(hist[0])\n",
    "        ccdf = np.cumsum(hist_norm[::-1])[::-1] \n",
    "        plt.plot(bins[:-1], ccdf, color='r', marker='o', label='Empirical data', alpha=0.5)\n",
    "        plt.bar(bins[:-1], ccdf, widths, bottom=10**(-4), alpha=0.5, label='Empirical histogram')\n",
    "        \n",
    "        \n",
    "        hist_zipf = np.histogram(zipfs_law_y, bins=bins)\n",
    "        hist_zipf_norm = hist_zipf[0]/sum(hist_zipf[0])\n",
    "        ccdf_zipf = np.cumsum(hist_zipf_norm[::-1])[::-1] \n",
    "        plt.plot(bins[:-1], ccdf_zipf, color='green', marker='o', label='Zipf\\'s law', alpha=0.5)\n",
    "          \n",
    "        n_indegree = len(indegree)\n",
    "        n_power_law = len(indegree[indegree>=xmin_PL])\n",
    "        n_power_law_reduced = len(indegree[indegree>=xmin_PL_reduced])\n",
    "        n_samples = 1000000\n",
    "    \n",
    "        plt.loglog(np.linspace(xmin_PL,xmax, 200), powerlaw_fit_distribution.ccdf(np.linspace(xmin_PL,xmax, 200))*n_power_law/n_indegree, label='fit ccdf: alpha=' +str(powerlawfit.power_law.alpha)+' sigma='+str(powerlawfit.power_law.sigma)+' xmin='+str(int(powerlawfit.power_law.xmin)), alpha=0.5)\n",
    "        plt.loglog(np.linspace(xmin_PL_reduced,xmax, 200), powerlaw_fit_reduced_distribution.ccdf(np.linspace(xmin_PL_reduced,xmax, 200))*n_power_law_reduced/n_indegree, label='fit ccdf: alpha=' +str(powerlawfit_reduced.power_law.alpha)+' sigma='+str(powerlawfit_reduced.power_law.sigma)+' xmin='+str(int(powerlawfit_reduced.power_law.xmin)), alpha=0.5)\n",
    "    \n",
    "        sigma, loc, scale = stats.lognorm.fit(indegree, floc=0)\n",
    "        logn_samples = np.random.lognormal(np.log(scale), sigma, n_samples)\n",
    "        hist_logn = np.histogram(logn_samples, bins=bins)\n",
    "        hist_logn_norm = hist_logn[0]/sum(hist_logn[0])\n",
    "        logn_ccdf = np.cumsum(hist_logn_norm[::-1])[::-1]\n",
    "        plt.loglog(bins[:-1], logn_ccdf, alpha=0.5, label='lognormal fit mu='+str(np.log(scale))+' sigma='+str(sigma))\n",
    "        \n",
    "        plt.title('Indegree Complementary Cumulative Distribution Function and power-law fit, normalized after xmin')\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.xlabel('indegree')\n",
    "        plt.ylabel('ccdf')\n",
    "        if (save_plot):\n",
    "            tikz.save(self.folder_name+'indegree_ccdf.tikz', table_row_sep='\\\\\\\\\\n')\n",
    "        plt.show()\n",
    "    \n",
    "    # Out-degree analysis functions:\n",
    "    \n",
    "     # The following function plots the outdegree distribution, as reported in Fig. 8\n",
    "    def plot_outdegree_distribution(self, save_plot):\n",
    "        outdegree_df = self.followers_df['Outdegree'].value_counts().rename_axis('Outdegree').reset_index(name='Count')\n",
    "        outdegree_df = outdegree_df.append({'Outdegree': 0, 'Count': 0}, ignore_index=True)\n",
    "        outdegree_df.sort_values('Outdegree', inplace=True, ascending=True)\n",
    "     \n",
    "        outdegree_df['Count'] = outdegree_df['Count'].divide(sum(outdegree_df['Count']))\n",
    "        outdegree_df['Sum_count'] = 1-np.cumsum(outdegree_df['Count'])\n",
    "        outdegree_df['Outdegreeplusone'] = outdegree_df['Outdegree']+1\n",
    "        \n",
    "        fig, ax = plt.subplots()\n",
    "        ax = outdegree_df.plot(x= 'Outdegreeplusone', y ='Sum_count', kind='line', label='Data', ax=ax)\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.xlim([1, 150])\n",
    "        plt.xscale('log')\n",
    "        plt.yscale('log')\n",
    "        plt.ylim([10**-7, 1])\n",
    "        plt.xlabel('Outdegree')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('{} Nodes outdegree distribution'.format(self.name))\n",
    "        if (save_plot):\n",
    "            tikz.save(self.folder_name+'outdegree_ccdf.tikz', table_row_sep='\\\\\\\\\\n')\n",
    "    \n",
    "    # The next function computes the requested percentile of the empirical out-degree distribution\n",
    "    def compute_outdegree_percentile(self, percentile):\n",
    "        return np.percentile(game.followers_df['Outdegree'].tolist(), percentile)\n",
    "            \n",
    "    # The next function computes the out-degree distribution of the followers of the \"n_top_agents\" users.\n",
    "    def compute_followers_outdegree_distribution(self, n_top_agents):\n",
    "        broadcasters_id = tuple(self.nodes_df[0:n_top_agents]['Id']) \n",
    "        followers_outdegree_distribution = [[0] * (max(self.followers_df['Outdegree'])+1) for _ in range(len(broadcasters_id))]\n",
    "        \n",
    "        for broadcaster, f in zip(broadcasters_id, followers_outdegree_distribution):\n",
    "            followers_list = self.edges_df.loc[self.edges_df['Target'] == broadcaster, 'Source'].tolist()\n",
    "            for follower in followers_list:\n",
    "                outdegree = self.outdegree_struct.get(follower)\n",
    "                f[outdegree-1] = f[outdegree-1]+1 \n",
    "                #storing the count for outdegree i into the cell i-1, given that for i=0 the count is 0.\n",
    "        return followers_outdegree_distribution\n",
    "        \n",
    "    # The next function provides the plot of the out-degree distribution, aggregated per each of the first n_top_agents broadcasters\n",
    "    def plot_outdegree_distribution_stacked(self, n_top_agents, save_plot):    \n",
    "        followers_outdegree_distribution = self.compute_followers_outdegree_distribution(n_top_agents)\n",
    "        # We only consider those followers with outdegree of at most 20 (which coincides with more than 99% of the followers)\n",
    "        for ele in followers_outdegree_distribution:\n",
    "            del ele[20:]\n",
    "              \n",
    "        followers_outdegree_distribution_normalized = [[j / sum(i) if sum(i) != 0 else 0 for j in i] for i in followers_outdegree_distribution]\n",
    "    \n",
    "        df = pd.DataFrame(followers_outdegree_distribution, index=list(range(1, n_top_agents + 1)))\n",
    "        df_norm = pd.DataFrame(followers_outdegree_distribution_normalized, index=list(range(1, n_top_agents + 1)))\n",
    "\n",
    "        plt.figure(0)\n",
    "        df_norm.plot(kind='bar', stacked='True', legend=None)\n",
    "        plt.xlabel('Rank')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title(self.name + ':#Followers')\n",
    "        plt.legend(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10'], title='Outdegree')\n",
    "        if (save_plot):\n",
    "            tikz.save(self.folder_name+'followers_outdegree_dist_normalized.tikz', table_row_sep='\\\\\\\\\\n')\n",
    "\n",
    "                \n",
    "    # Overlap analysis functions:\n",
    "    \n",
    "    # The next function computes the overlap matrix, as defined in the manuscript\n",
    "    def create_heatmap(self, n_top_agents):\n",
    "        heatmap_new = np.zeros([n_top_agents+1,n_top_agents+1])\n",
    "        broadcasters_id = tuple(self.nodes_df[0:n_top_agents+1]['Id'])\n",
    "        n_i =0\n",
    "        for i in broadcasters_id:\n",
    "            n_j = 0\n",
    "            row_i = []\n",
    "            i_followers = self.edges_df.loc[self.edges_df['Target'] == i, 'Source'].tolist()\n",
    "            i_followers = set(i_followers)\n",
    "            for j in broadcasters_id:\n",
    "                j_followers = self.edges_df.loc[self.edges_df['Target'] == j, 'Source'].tolist()\n",
    "                j_followers = set(j_followers)\n",
    "                row_i.append(len(j_followers & i_followers) / len(i_followers))\n",
    "                heatmap_new[n_i, n_j] = len(j_followers & i_followers) / len(i_followers)\n",
    "                n_j = n_j +1\n",
    "            n_i = n_i+1\n",
    "        return heatmap_new\n",
    "    \n",
    "    # The next function plots (and saves) the overlap matrix, as defined in the manuscript.\n",
    "    def plot_heatmap(self, size, save_plot):\n",
    "        heatmap = self.create_heatmap(size)\n",
    "        y, x = np.mgrid[slice(0, size+1, 1), slice(0, size+1, 1)]\n",
    "        plt.pcolor(x, y, heatmap[0:size+1, 0:size+1], cmap=scaled_cmap(cm.hot, 2))\n",
    "        plt.xlim(0,size)\n",
    "        plt.ylim(0,size)\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.colorbar()\n",
    "        if (save_plot):\n",
    "            dict = {'x': (np.reshape(x+0.5, ((size+1)**2),1)), 'y': (np.reshape(y+0.5, ((size+1)**2),1)), 'c': (np.reshape(heatmap, ((size+1)**2),1))}\n",
    "            df = pd.DataFrame(dict)\n",
    "            df.to_csv(self.folder_name+'overlap_matrix.csv', index=False, sep ='\\t')\n",
    "        plt.xlabel('Rank j')\n",
    "        plt.ylabel('Rank i')\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the analysis of the Chess data-set, uncomment the following three lines\n",
    "# name = 'Chess' # --> the name of the database (without .db) NOTE: The database must be saved in the same folder\n",
    "# folder_name = 'Chess_Results/'\n",
    "# id_remove_list = [158881542]\n",
    "\n",
    "# For the analysis of the Poker data-set, uncomment the following three lines\n",
    "# name = 'Poker' # --> the name of the database (without .db) NOTE: The database must be saved in the same folder\n",
    "# folder_name = 'Poker_Results/'\n",
    "# id_remove_list = [55481471] \n",
    "\n",
    "# Use the following line to change the filtering criterion on the interest-index (or dedication)\n",
    "filtering = {'dedication': 0.8, 'final_date' :'2020-10-01'}\n",
    "game = Game(name, folder_name, filtering)\n",
    "\n",
    "# For each of the chess and poker data-sets, two user-ids were removed after manual inspection of the content of the channels.\n",
    "# Apparently, even though these users were recently streaming in these categories, much of their followers had very poor overlap with the followers of others,\n",
    "# which indicates that their followers are due to a different (possibly older) interest.\n",
    "game.fetch_data(id_remove_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we print some basics statistics related to the data-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game.print_statistics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zipf's law\n",
    "The following code provides the results reported in Fig. 4 in the manuscript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top_agents = 15\n",
    "save_plot = True\n",
    "game.plot_Zipf(n_top_agents, save_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-degree distribution\n",
    "In the following, we provide the code for the in-degree distribution analysis shown in Supplementary Fig. 12. First, we show the probability density function, and the various fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_plot = True\n",
    "game.plot_indegree_pdf(save_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we show the complementary cumulative distribution function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_plot = True\n",
    "game.plot_indegree_ccdf(save_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Out-degree distribution\n",
    "The following code provides the results reported in Fig. 8 in the manuscript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_plot = True\n",
    "game.plot_outdegree_distribution(save_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also compute the 99th percentile of the distribution, as reported in the manuscript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game.compute_outdegree_percentile(99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we provide the plot of the out-degree distribution, aggregated per each followers. The number of followers is normalized with the total number of followers of the broadcaster. The data are reported in Supplementary Fig. 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top_agents = 15\n",
    "save_plot = True\n",
    "game.plot_outdegree_distribution_stacked(n_top_agents, save_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overlap analysis\n",
    "The following code provides the results reported in Fig. 10b, and in Supplementary Figs. 10, 11, 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_plot = True\n",
    "n_top_agents = 15\n",
    "game.plot_heatmap(n_top_agents, save_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
